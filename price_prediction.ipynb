{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nazimulrahmann/house_price_prediction/blob/main/price_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPbJq29q3u6a"
      },
      "source": [
        "**Importing Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "kaHUh-eOKUdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IQzIqwmK2G8g"
      },
      "outputs": [],
      "source": [
        "# === Suppress warnings ===\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Suppress warnings in notebook output\n",
        "\n",
        "# === Typing and datetime ===\n",
        "from typing import Union, Tuple, Dict, List, Optional  # For type hints and static typing in function definitions\n",
        "from datetime import datetime  # To handle date and time data\n",
        "\n",
        "# === Data manipulation ===\n",
        "import numpy as np  # For numerical computing and array handling\n",
        "import pandas as pd  # For handling tabular data using DataFrames\n",
        "\n",
        "# === Visualization ===\n",
        "import matplotlib.pyplot as plt  # Basic plotting\n",
        "import seaborn as sns  # Statistical visualizations with better aesthetics\n",
        "\n",
        "# === Statistical tools ===\n",
        "from scipy import stats  # General statistical functions and tests\n",
        "from scipy.stats import zscore, randint, loguniform, uniform  # Specific functions for normalization and sampling\n",
        "\n",
        "# === Scikit-learn core tools for modeling ===\n",
        "from sklearn.model_selection import train_test_split  # Split data into training and testing sets\n",
        "from sklearn.model_selection import GridSearchCV  # Exhaustive hyperparameter tuning using cross-validation\n",
        "from sklearn.model_selection import KFold, cross_val_score  # K-Fold cross-validation and scoring models\n",
        "\n",
        "# === Evaluation Metrics ===\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,  # Measures average squared difference between estimated and actual values\n",
        "    mean_absolute_error,  # Measures average absolute difference\n",
        "    r2_score,  # R-squared (coefficient of determination)\n",
        "    explained_variance_score,  # How well future samples are likely to be explained\n",
        "    max_error,  # Maximum residual error\n",
        "    mean_absolute_percentage_error  # MAE as a percentage of true values\n",
        ")\n",
        "\n",
        "# === Preprocessing utilities ===\n",
        "from sklearn.impute import SimpleImputer  # Fill missing values using mean/median/mode etc.\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler,  # Standardize features (mean=0, std=1)\n",
        "    MinMaxScaler,  # Scale features to a given range (default 0 to 1)\n",
        "    RobustScaler,  # Scale features using statistics robust to outliers\n",
        "    OneHotEncoder,  # Convert categorical variables into binary vectors\n",
        "    OrdinalEncoder  # Encode categorical features as integer values\n",
        ")\n",
        "\n",
        "# === Feature engineering ===\n",
        "from sklearn.feature_selection import (\n",
        "    SelectKBest,  # Select top k features based on a scoring function\n",
        "    f_regression,  # Scoring function used in regression-based feature selection\n",
        "    VarianceThreshold  # Remove features with low variance (constant features)\n",
        ")\n",
        "\n",
        "# === Pipelines and column transformation ===\n",
        "from sklearn.pipeline import Pipeline, make_pipeline  # Combine preprocessing and modeling steps\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer  # Handle preprocessing for different column types\n",
        "\n",
        "# === Regression models ===\n",
        "\n",
        "# --- Linear Models ---\n",
        "from sklearn.linear_model import (\n",
        "    LinearRegression,  # Ordinary least squares regression\n",
        "    Ridge,  # L2 regularized linear regression\n",
        "    Lasso,  # L1 regularized linear regression (feature selection)\n",
        "    ElasticNet,  # Combination of L1 and L2 penalties\n",
        "    BayesianRidge,  # Bayesian linear regression\n",
        "    ARDRegression,  # Bayesian regression with automatic relevance determination\n",
        "    SGDRegressor,  # Linear model fitted by stochastic gradient descent\n",
        "    HuberRegressor,  # Robust to outliers using Huber loss\n",
        "    TheilSenRegressor,  # Robust linear model using median slopes\n",
        "    RANSACRegressor,  # Fits a model robust to outliers\n",
        "    PassiveAggressiveRegressor,  # Online learning algorithm\n",
        "    OrthogonalMatchingPursuit  # Sparse linear regression\n",
        ")\n",
        "\n",
        "# --- Tree-based Models ---\n",
        "from sklearn.tree import DecisionTreeRegressor  # Non-linear regression using decision trees\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestRegressor,  # Ensemble of decision trees with averaging\n",
        "    GradientBoostingRegressor,  # Boosted ensemble of trees for better accuracy\n",
        "    AdaBoostRegressor,  # Boosting technique using weighted averages\n",
        "    ExtraTreesRegressor,  # Similar to Random Forest but with more randomness\n",
        "    BaggingRegressor,  # Bagging ensemble method\n",
        "    StackingRegressor,  # Combines multiple models using a meta-model\n",
        "    VotingRegressor,  # Averages predictions from multiple models\n",
        "    HistGradientBoostingRegressor  # Faster implementation of Gradient Boosting with histogram binning\n",
        ")\n",
        "\n",
        "# --- Support Vector Machines ---\n",
        "from sklearn.svm import SVR, LinearSVR, NuSVR  # Regression using support vector machines\n",
        "\n",
        "# --- Nearest Neighbors ---\n",
        "from sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor  # Instance-based regressors\n",
        "\n",
        "# --- Neural Networks ---\n",
        "from sklearn.neural_network import MLPRegressor  # Multi-layer perceptron for regression\n",
        "\n",
        "# --- Gaussian Processes ---\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor  # Non-parametric kernel-based probabilistic model\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, RationalQuadratic  # Kernels used in Gaussian Processes\n",
        "\n",
        "# --- Other regressors ---\n",
        "from sklearn.kernel_ridge import KernelRidge  # Ridge regression with kernels\n",
        "from sklearn.cross_decomposition import PLSRegression  # Partial Least Squares Regression\n",
        "from sklearn.isotonic import IsotonicRegression  # Non-linear regression that preserves order\n",
        "from sklearn.dummy import DummyRegressor  # Baseline model for comparison\n",
        "from sklearn.compose import TransformedTargetRegressor  # Transform target variable during training\n",
        "\n",
        "# === Advanced ensemble models from external libraries ===\n",
        "from xgboost import XGBRegressor  # Gradient boosting model from XGBoost\n",
        "from lightgbm import LGBMRegressor  # Gradient boosting from LightGBM\n",
        "from catboost import CatBoostRegressor  # Gradient boosting from CatBoost, handles categorical variables natively\n",
        "\n",
        "# === Model stacking ===\n",
        "from mlxtend.regressor import StackingCVRegressor  # Cross-validated stacking of multiple regressors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE5fRcHl4q9q"
      },
      "source": [
        "**Data Loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G_EYfLsHiMC9"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('/content/house_price_train_data.csv')\n",
        "test_df = pd.read_csv('/content/house_price_test_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhaC1QqD40uV"
      },
      "outputs": [],
      "source": [
        "print(f\"Train data shape: {train_df.shape}\")\n",
        "print(f\"Test data shape: {test_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDFH5i6Q5BiT"
      },
      "source": [
        "**Data Overview**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0wec-pE6qld"
      },
      "outputs": [],
      "source": [
        "def safe_sample(train_df: pd.DataFrame, num: int):\n",
        "    # Corrected: Use the passed argument train_df instead of an undefined variable df\n",
        "    return train_df.sample(min(num, len(train_df)), random_state=42)\n",
        "\n",
        "# Display first 3 rows of the DataFrame (or fewer if there aren't enough rows)\n",
        "print(\"\\nFirst 3 rows:\")\n",
        "display(train_df.head(3))\n",
        "\n",
        "# Display last 3 rows\n",
        "print(\"\\nLast 3 rows:\")\n",
        "display(train_df.tail(3))\n",
        "\n",
        "# Display 3 random rows\n",
        "print(\"\\nRandom 3 rows:\")\n",
        "display(safe_sample(train_df, num=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Sb4tYAA8wUt"
      },
      "outputs": [],
      "source": [
        "# Getting info form data\n",
        "train_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Columns Analysis**"
      ],
      "metadata": {
        "id": "IJm9-qe7WoZv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPctmkZw9ZOc"
      },
      "outputs": [],
      "source": [
        "def column_types_analysis(df: pd.DataFrame):\n",
        "    \"\"\"Analyze and visualize column data types distribution.\n",
        "\n",
        "    Args:\n",
        "        df: pandas DataFrame to analyze\n",
        "    \"\"\"\n",
        "    print(\"=\"*50)\n",
        "    print(\"COLUMN DATA TYPES ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    type_counts = df.dtypes.value_counts()\n",
        "    display(type_counts)\n",
        "\n",
        "    # Pass figsize as a tuple (width, height)\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    type_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90,\n",
        "                    colors=sns.color_palette('pastel'))\n",
        "    plt.title('Distribution of Column Data Types')\n",
        "    plt.ylabel('')\n",
        "    plt.show()\n",
        "# Run Function\n",
        "column_types_analysis(df=train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unique Values Analysis**"
      ],
      "metadata": {
        "id": "bPoGQrTjWuaF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ-Faqs-9uIX"
      },
      "outputs": [],
      "source": [
        "def unique_values_analysis(df: pd.DataFrame,\n",
        "                           columns: Optional[Union[str, List[str]]] = None,\n",
        "                           plot: bool = True,\n",
        "                           max_unique_plot: int = 20) -> None:\n",
        "    \"\"\"\n",
        "    Analyze and visualize unique values in specified DataFrame columns.\n",
        "\n",
        "    Args:\n",
        "        df: pandas DataFrame to analyze.\n",
        "        columns: Column(s) to analyze. If None, all columns are used.\n",
        "        plot: Whether to generate visualizations.\n",
        "        max_unique_plot: Max number of unique values to plot per column.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*50)\n",
        "    print(\"UNIQUE VALUES ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # If columns is None, analyze all columns\n",
        "    if columns is None:\n",
        "        columns = df.columns.tolist()\n",
        "    # Ensure columns is a list for iteration, even if a single column name was passed as a string\n",
        "    elif isinstance(columns, str):\n",
        "        columns = [columns]\n",
        "\n",
        "    # Filter only valid columns that exist in the DataFrame\n",
        "    columns = [col for col in columns if col in df.columns]\n",
        "\n",
        "    # Check if any valid columns were found\n",
        "    if not columns:\n",
        "        print(\"No valid columns found to analyze.\")\n",
        "        return\n",
        "\n",
        "    # Calculate number of unique values per column, sort them in descending order\n",
        "    unique_counts = df[columns].nunique(dropna=False).sort_values(ascending=False)\n",
        "\n",
        "    print(\"\\nNumber of Unique Values per Column:\")\n",
        "    print(unique_counts)\n",
        "\n",
        "    if plot:\n",
        "        # Filter columns that have a reasonable number of unique values to plot\n",
        "        # Also ensure the column exists in the original df to avoid KeyError later\n",
        "        cols_to_plot = [col for col in unique_counts[unique_counts <= max_unique_plot].index if col in df.columns]\n",
        "\n",
        "\n",
        "        for col in cols_to_plot:\n",
        "            plt.figure(figsize=(10, 5))  # Set figure size within the function\n",
        "\n",
        "            # For categorical or low-cardinality columns\n",
        "            if df[col].dtype in ['object', 'category'] or unique_counts[col] <= 10:\n",
        "                # Get value counts including NaN, normalize to percentage\n",
        "                value_counts = df[col].value_counts(normalize=True, dropna=False) * 100\n",
        "\n",
        "                # Truncate to top N if too many unique categories\n",
        "                if len(value_counts) > max_unique_plot:\n",
        "                    value_counts = value_counts.head(max_unique_plot)\n",
        "                    title_suffix = f\" (Top {max_unique_plot})\"\n",
        "                else:\n",
        "                    title_suffix = \"\"\n",
        "\n",
        "                # Bar plot for categorical distribution\n",
        "                value_counts.plot(kind='bar', color='skyblue')\n",
        "                plt.title(f'Distribution of {col}{title_suffix}')\n",
        "                plt.ylabel('Percentage')\n",
        "                plt.xticks(rotation=45)\n",
        "\n",
        "            # For numerical columns\n",
        "            else:\n",
        "                # Drop NaN for plotting distributions\n",
        "                numeric_data = df[col].dropna()\n",
        "\n",
        "                # Use seaborn histogram with KDE (kernel density estimate)\n",
        "                sns.histplot(numeric_data, kde=True, color='skyblue', bins=30)\n",
        "                plt.title(f'Distribution of {col}')\n",
        "                plt.xlabel(col)\n",
        "                plt.ylabel('Frequency')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "# Usage\n",
        "unique_values_analysis(train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statistical Analysis**"
      ],
      "metadata": {
        "id": "9DhnEyOtW1oH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whcr6dOrs-FL"
      },
      "outputs": [],
      "source": [
        "def describe_data(df: pd.DataFrame, include_all: bool = False) -> None:\n",
        "    \"\"\"\n",
        "    Generate and display descriptive statistics for a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to analyze.\n",
        "        include_all (bool): If True, include all columns (numeric + categorical) in the statistics.\n",
        "                            If False, show numeric and categorical stats separately.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"DESCRIPTIVE STATISTICS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # If DataFrame is empty, print a message and exit\n",
        "    if df.empty:\n",
        "        print(\"The DataFrame is empty. No statistics to display.\")\n",
        "        return\n",
        "\n",
        "    if include_all:\n",
        "        # Show descriptive stats for all column types\n",
        "        print(df.describe(include='all').to_string())\n",
        "    else:\n",
        "        # Select numeric columns\n",
        "        numeric_cols = df.select_dtypes(include=np.number).columns\n",
        "\n",
        "        if not numeric_cols.empty:\n",
        "            print(\"\\nNumeric Columns Statistics:\")\n",
        "            # Describe numeric columns and print as a string for readability\n",
        "            print(df[numeric_cols].describe().to_string())\n",
        "        else:\n",
        "            print(\"\\nNo numeric columns found.\")\n",
        "\n",
        "        # Select categorical (object or category) columns\n",
        "        cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "        if not cat_cols.empty:\n",
        "            print(\"\\nCategorical Columns Statistics:\")\n",
        "            # Describe categorical columns and print as a string\n",
        "            print(df[cat_cols].describe().to_string())\n",
        "        else:\n",
        "            print(\"\\nNo categorical columns found.\")\n",
        "\n",
        "# Describe numeric and categorical separately\n",
        "describe_data(train_df)\n",
        "\n",
        "# Describe all columns at once\n",
        "describe_data(train_df, include_all=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE6NKrdVChNN"
      },
      "source": [
        "**Univariate Numerical Column Analysis**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub_WK2_pLqOD"
      },
      "outputs": [],
      "source": [
        "def numerical_cols_analysis(df, figsize=(15, 10), bins=30, color='skyblue'):\n",
        "    \"\"\"\n",
        "    Perform univariate analysis on all numerical columns in a DataFrame\n",
        "    with multiple visualization types.\n",
        "\n",
        "    Parameters:\n",
        "    - df: Pandas DataFrame\n",
        "    - figsize: Tuple for figure size\n",
        "    - bins: Number of bins for histograms\n",
        "    - color: Color for plots\n",
        "\n",
        "    Returns:\n",
        "    - Dictionary containing skewness and kurtosis for each column\n",
        "    \"\"\"\n",
        "\n",
        "    # Select only numerical columns\n",
        "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    if len(numerical_cols) == 0:\n",
        "        print(\"No numerical columns found in the DataFrame.\")\n",
        "        return None\n",
        "\n",
        "    stats_results = {}\n",
        "\n",
        "    for col in numerical_cols:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"UNIVARIATE ANALYSIS FOR: {col}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Create figure with subplots\n",
        "        fig, axes = plt.subplots(2, 3, figsize=figsize)\n",
        "        fig.suptitle(f'Univariate Analysis: {col}', fontsize=16)\n",
        "\n",
        "        # Data summary\n",
        "        col_data = df[col].dropna()\n",
        "        n_missing = df[col].isna().sum()\n",
        "        stats_results[col] = {\n",
        "            'count': len(col_data),\n",
        "            'missing': n_missing,\n",
        "            'mean': col_data.mean(),\n",
        "            'median': col_data.median(),\n",
        "            'min': col_data.min(),\n",
        "            'max': col_data.max(),\n",
        "            'std': col_data.std(),\n",
        "            'skew': col_data.skew(),\n",
        "            'kurtosis': col_data.kurtosis()\n",
        "        }\n",
        "\n",
        "        print(f\"\\nSummary Statistics:\")\n",
        "        print(f\"- Missing values: {n_missing} ({n_missing/len(df):.1%})\")\n",
        "        print(f\"- Mean: {stats_results[col]['mean']:.2f}\")\n",
        "        print(f\"- Median: {stats_results[col]['median']:.2f}\")\n",
        "        print(f\"- Range: {stats_results[col]['min']:.2f} to {stats_results[col]['max']:.2f}\")\n",
        "\n",
        "        # Box plot\n",
        "        sns.boxplot(x=df[col], color=color, ax=axes[0, 1])\n",
        "        axes[0, 1].set_title(f'Box Plot of {col}')\n",
        "\n",
        "        # Q-Q plot\n",
        "        stats.probplot(df[col].dropna(), dist=\"norm\", plot=axes[1, 0])\n",
        "        axes[1, 0].set_title(f'Q-Q Plot of {col}')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return stats_results\n",
        "    # Run the analysis\n",
        "numerical_cols_analysis(train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Univariate Categorical Column Analysis**"
      ],
      "metadata": {
        "id": "vfExFx7lW82Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLmgU7deWuOR"
      },
      "outputs": [],
      "source": [
        "def categorical_univariate_analysis(df, figsize=(15, 10), top_n=20,\n",
        "                                  color='skyblue', palette='viridis'):\n",
        "    \"\"\"\n",
        "    Perform univariate analysis on all categorical columns in a DataFrame\n",
        "    with multiple visualization types.\n",
        "\n",
        "    Parameters:\n",
        "    - df: Pandas DataFrame\n",
        "    - figsize: Tuple for figure size\n",
        "    - top_n: Show top N categories (for high-cardinality features)\n",
        "    - color: Color for single-color plots\n",
        "    - palette: Color palette for multi-color plots\n",
        "\n",
        "    Returns:\n",
        "    - Dictionary containing summary statistics for each column\n",
        "    \"\"\"\n",
        "\n",
        "    # Select categorical columns (object, category, bool types)\n",
        "    categorical_cols = df.select_dtypes(include=['object', 'category', 'bool']).columns\n",
        "\n",
        "    if len(categorical_cols) == 0:\n",
        "        print(\"No categorical columns found in the DataFrame.\")\n",
        "        return None\n",
        "\n",
        "    stats_results = {}\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"CATEGORICAL ANALYSIS FOR: {col}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Create figure with subplots\n",
        "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
        "        fig.suptitle(f'Categorical Analysis: {col}', fontsize=16)\n",
        "\n",
        "        # Data summary\n",
        "        # Convert 'object' type columns to 'category' for better handling\n",
        "        col_data = df[col].astype('category') if df[col].dtype == 'object' else df[col]\n",
        "        value_counts = col_data.value_counts(dropna=False)\n",
        "        value_counts_pct = col_data.value_counts(dropna=False, normalize=True)\n",
        "        n_unique = col_data.nunique()\n",
        "        n_missing = col_data.isna().sum()\n",
        "\n",
        "        stats_results[col] = {\n",
        "            'count': len(col_data),\n",
        "            'missing': n_missing,\n",
        "            'missing_pct': n_missing / len(col_data),\n",
        "            'n_unique': n_unique,\n",
        "            'value_counts': value_counts,\n",
        "            'value_counts_pct': value_counts_pct\n",
        "        }\n",
        "\n",
        "        print(f\"\\nSummary Statistics:\")\n",
        "        print(f\"- Missing values: {n_missing} ({n_missing/len(df):.1%})\")\n",
        "        print(f\"- Unique values: {n_unique}\")\n",
        "        if n_unique <= 20:\n",
        "            print(\"\\nValue Counts:\")\n",
        "            print(value_counts.to_string())\n",
        "            print(\"\\nValue Percentages:\")\n",
        "            print(value_counts_pct.apply(lambda x: f\"{x:.1%}\").to_string())\n",
        "        else:\n",
        "            print(f\"\\nTop {top_n} Values (of {n_unique}):\")\n",
        "            print(value_counts.head(top_n).to_string())\n",
        "            print(\"\\nTop Value Percentages:\")\n",
        "            print(value_counts_pct.head(top_n).apply(lambda x: f\"{x:.1%}\").to_string())\n",
        "\n",
        "        # Handle high cardinality (show only top_n categories)\n",
        "        plot_data = col_data.copy()\n",
        "        if n_unique > top_n:\n",
        "            top_categories = value_counts.index[:top_n].tolist() # Get top categories as a list\n",
        "            # Add 'Other' to categories *before* using .where()\n",
        "            if 'Other' not in plot_data.cat.categories:\n",
        "                 plot_data = plot_data.cat.add_categories('Other')\n",
        "            plot_data = plot_data.where(plot_data.isin(top_categories), 'Other')\n",
        "\n",
        "\n",
        "        # Count Plot (Bar plot)\n",
        "        if n_unique > 15:\n",
        "            # Horizontal for many categories\n",
        "            sns.countplot(y=plot_data, order=plot_data.value_counts().index,\n",
        "                         color=color, ax=axes[0, 0])\n",
        "        else:\n",
        "            sns.countplot(x=plot_data, order=plot_data.value_counts().index,\n",
        "                         color=color, ax=axes[0, 0])\n",
        "        axes[0, 0].set_title(f'Count Plot of {col}')\n",
        "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Pie Chart (only if reasonable number of categories)\n",
        "        if n_unique <= 10 and n_unique > 1:\n",
        "            plot_data.value_counts().plot.pie(autopct='%1.1f%%',\n",
        "                                            colors=sns.color_palette(palette),\n",
        "                                            ax=axes[0, 1])\n",
        "            axes[0, 1].set_ylabel('')  # Remove y-label\n",
        "            axes[0, 1].set_title(f'Pie Chart of {col}')\n",
        "        else:\n",
        "            # If too many categories, show donut chart of top N\n",
        "            top_data = plot_data.value_counts().head(10)\n",
        "            axes[0, 1].pie(top_data, labels=top_data.index,\n",
        "                          autopct='%1.1f%%', pctdistance=0.85,\n",
        "                          colors=sns.color_palette(palette))\n",
        "            centre_circle = plt.Circle((0,0), 0.70, fc='white')\n",
        "            axes[0, 1].add_artist(centre_circle)\n",
        "            axes[0, 1].set_title(f'Donut Chart (Top {len(top_data)} Categories)')\n",
        "\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "    return stats_results\n",
        "# Run the analysis\n",
        "categorical_univariate_analysis(train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Missing Values Analysis and Imoutation**"
      ],
      "metadata": {
        "id": "0xInZeBbXL8b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJYWHi3d23Fd"
      },
      "outputs": [],
      "source": [
        "def handle_missing_values(df, impute=True, categorical_strategy='most_frequent', numerical_strategy='mean'):\n",
        "    \"\"\"\n",
        "    Analyze and handle missing values in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pandas DataFrame\n",
        "    - impute: bool, whether to impute missing values (default True)\n",
        "    - categorical_strategy: strategy for categorical imputation ('most_frequent', 'constant')\n",
        "    - numerical_strategy: strategy for numerical imputation ('mean', 'median', 'mode')\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with missing values imputed (if impute=True)\n",
        "    - Displays a bar plot of missing value percentages\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate missing value percentages\n",
        "    missing_percent = df.isnull().mean() * 100\n",
        "    missing_percent = missing_percent[missing_percent > 0].sort_values(ascending=False)\n",
        "\n",
        "    # Plot missing values\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=missing_percent.index, y=missing_percent.values, palette='viridis')\n",
        "    plt.axhline(y=30, color='red', linestyle='--', label='30% threshold')\n",
        "    plt.title('Percentage of Missing Values by Column')\n",
        "    plt.ylabel('Percentage Missing (%)')\n",
        "    plt.xlabel('Columns')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Total missing values before handling: {df.isnull().sum().sum()}\")\n",
        "\n",
        "    if not impute:\n",
        "        return df\n",
        "\n",
        "    # Make a copy of the original dataframe\n",
        "    df_imputed = df.copy()\n",
        "\n",
        "    # Separate categorical and numerical columns\n",
        "    categorical_cols = df_imputed.select_dtypes(include=['object', 'category']).columns\n",
        "    numerical_cols = df_imputed.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "    # Impute categorical columns\n",
        "    for col in categorical_cols:\n",
        "        if df_imputed[col].isnull().any():\n",
        "            if categorical_strategy == 'most_frequent':\n",
        "                df_imputed[col].fillna(df_imputed[col].mode()[0], inplace=True)\n",
        "            elif categorical_strategy == 'constant':\n",
        "                df_imputed[col].fillna('Missing', inplace=True)\n",
        "\n",
        "    # Impute numerical columns\n",
        "    for col in numerical_cols:\n",
        "        if df_imputed[col].isnull().any():\n",
        "            if numerical_strategy == 'mean':\n",
        "                df_imputed[col].fillna(df_imputed[col].mean(), inplace=True)\n",
        "            elif numerical_strategy == 'median':\n",
        "                df_imputed[col].fillna(df_imputed[col].median(), inplace=True)\n",
        "            elif numerical_strategy == 'mode':\n",
        "                df_imputed[col].fillna(df_imputed[col].mode()[0], inplace=True)\n",
        "\n",
        "    print(f\"Total missing values after imputation: {df_imputed.isnull().sum().sum()}\")\n",
        "\n",
        "    return df_imputed\n",
        "# Example usage:\n",
        "handle_missing_values(train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Outliers**"
      ],
      "metadata": {
        "id": "hqGJvALEXWM_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5BkfgEB9B5y"
      },
      "outputs": [],
      "source": [
        "def outliers_iqr(\n",
        "    data: pd.DataFrame,\n",
        "    columns: list = None,\n",
        "    threshold: float = 3.0,\n",
        "    show_summary: bool = False\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Remove only extreme outliers from a DataFrame using the IQR method.\n",
        "\n",
        "    Parameters:\n",
        "    - data: Input DataFrame\n",
        "    - columns: List of columns to process (None for all numeric columns)\n",
        "    - threshold: IQR multiplier for extreme outlier detection (default: 3.0)\n",
        "    - show_summary: If True, prints number of rows removed\n",
        "\n",
        "    Returns:\n",
        "    - Cleaned DataFrame with extreme outliers removed\n",
        "    \"\"\"\n",
        "    if not isinstance(data, pd.DataFrame) or data.empty:\n",
        "        raise ValueError(\"Input must be a non-empty DataFrame\")\n",
        "\n",
        "    df = data.copy()\n",
        "    if columns is None:\n",
        "        columns = df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    rows_to_remove = set()\n",
        "\n",
        "    for col in columns:\n",
        "        if col not in df.columns:\n",
        "            continue\n",
        "\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower = Q1 - threshold * IQR\n",
        "        upper = Q3 + threshold * IQR\n",
        "\n",
        "        outliers = df[(df[col] < lower) | (df[col] > upper)]\n",
        "        rows_to_remove.update(outliers.index)\n",
        "\n",
        "    cleaned_df = df.drop(index=list(rows_to_remove))\n",
        "\n",
        "    if show_summary:\n",
        "        print(f\"Removed {len(rows_to_remove)} extreme outliers\")\n",
        "        print(f\"Original shape: {df.shape}, New shape: {cleaned_df.shape}\")\n",
        "\n",
        "    return cleaned_df\n",
        "# Usage\n",
        "train_df = outliers_iqr(train_df, show_summary=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "275BL0oVCoNE"
      },
      "source": [
        "**Features Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eSYaMpFFClcP"
      },
      "outputs": [],
      "source": [
        "# Apply the transformations to both training and testing datasets\n",
        "for df in [train_df, test_df]:\n",
        "\n",
        "    # 1. Age-related features\n",
        "    df['HouseAge'] = df['YrSold'] - df['YearBuilt']\n",
        "    # Age of the house when sold; older houses may have lower prices unless well-maintained.\n",
        "\n",
        "    df['RemodelAge'] = df['YrSold'] - df['YearRemodAdd']\n",
        "    # Years since the last remodel; more recent remodels can positively affect price.\n",
        "\n",
        "    df['SinceRemodel'] = df['YearRemodAdd'] - df['YearBuilt']\n",
        "    # Time taken to remodel after initial construction; quick remodels might indicate upgrades.\n",
        "\n",
        "    # 2. Size-related features\n",
        "    df['TotalSF'] = df['1stFlrSF'] + df['2ndFlrSF'] + df['TotalBsmtSF']\n",
        "    # Total square footage including finished basement; overall size is a key price factor.\n",
        "\n",
        "    df['TotalFinishedSF'] = df['1stFlrSF'] + df['2ndFlrSF'] + df['BsmtFinSF1'] + df['BsmtFinSF2']\n",
        "    # Total finished space in the house (excluding unfinished basement area).\n",
        "\n",
        "    df['SFPerRoom'] = df['GrLivArea'] / (df['TotRmsAbvGrd'] + df['FullBath'] + 0.5 * df['HalfBath'] + 1)\n",
        "    # Average square footage per room, a measure of room spaciousness.\n",
        "    # +1 to denominator to avoid division by zero.\n",
        "\n",
        "    df['TotalArea'] = df['GrLivArea'] + df['TotalBsmtSF']\n",
        "    # Total usable area including basement.\n",
        "\n",
        "    # 3. Bathroom features\n",
        "    df['TotalBaths'] = df['FullBath'] + 0.5 * df['HalfBath'] + df['BsmtFullBath'] + 0.5 * df['BsmtHalfBath']\n",
        "    # Total bathroom count including basement, with half baths weighted at 0.5.\n",
        "\n",
        "    df['BathRatio'] = (df['FullBath'] + df['BsmtFullBath']) / (df['TotalBaths'] + 0.01)\n",
        "    # Proportion of full baths out of total baths. Add 0.01 to avoid division by zero.\n",
        "\n",
        "    # 4. Porch/outdoor features\n",
        "    df['TotalPorchSF'] = df['OpenPorchSF'] + df['3SsnPorch'] + df['EnclosedPorch'] + df['ScreenPorch'] + df['WoodDeckSF']\n",
        "    # Total outdoor/porch area including all types.\n",
        "\n",
        "    df['PorchRatio'] = df['TotalPorchSF'] / (df['LotArea'] + 1)\n",
        "    # Proportion of lot used for porch or deck. Normalized by lot size.\n",
        "\n",
        "    # 5. Efficiency metrics\n",
        "    df['LivingEfficiency'] = df['GrLivArea'] / df['TotalSF']\n",
        "    # Ratio of above-ground living area to total space; a measure of how well the space is utilized.\n",
        "\n",
        "    df['BasementFinishRatio'] = (df['BsmtFinSF1'] + df['BsmtFinSF2']) / (df['TotalBsmtSF'] + 1)\n",
        "    # Portion of basement that is finished. +1 to avoid divide-by-zero errors.\n",
        "\n",
        "    # 6. Interaction features\n",
        "    df['AgeTimesQuality'] = df['HouseAge'] * df['OverallQual']\n",
        "    # Captures combined effect of age and quality. Newer high-quality houses vs. old lower-quality ones.\n",
        "\n",
        "    df['SizeTimesQuality'] = df['TotalSF'] * (df['OverallQual'] / 10)\n",
        "    # Blends size and quality into one feature. Normalized quality for scaling.\n",
        "\n",
        "# 🔻 Drop less informative or now-redundant features\n",
        "cols_to_drop = [\n",
        "    'Id',                # Unique ID, not predictive\n",
        "    'YrSold',            # Already used to calculate age features\n",
        "    'YearBuilt',         # Used in HouseAge and SinceRemodel\n",
        "    'YearRemodAdd',      # Used in RemodelAge\n",
        "    '1stFlrSF', '2ndFlrSF', 'BsmtFinSF1', 'BsmtFinSF2',\n",
        "    # Used in derived features like TotalSF, TotalFinishedSF\n",
        "\n",
        "    'OpenPorchSF', '3SsnPorch', 'EnclosedPorch', 'ScreenPorch', 'WoodDeckSF',\n",
        "    # Used in TotalPorchSF\n",
        "\n",
        "    'BsmtFullBath', 'BsmtHalfBath', 'HalfBath'\n",
        "    # Used in TotalBaths and BathRatio\n",
        "]\n",
        "\n",
        "# Drop columns from training and testing datasets\n",
        "train_df = train_df.drop(columns=cols_to_drop)\n",
        "test_df = test_df.drop(columns=[col for col in cols_to_drop if col in test_df.columns])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correlation Analysis**"
      ],
      "metadata": {
        "id": "DiUPXc_JXj-E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWgr4wTACwui"
      },
      "outputs": [],
      "source": [
        "# Compute correlation matrix (numeric columns only)\n",
        "corr_matrix = train_df.corr(numeric_only=True)\n",
        "\n",
        "# Sort correlations by absolute correlation with target ('SalePrice')\n",
        "target = 'SalePrice'\n",
        "sorted_features = corr_matrix[target].abs().sort_values(ascending=False).index\n",
        "sorted_corr = corr_matrix.loc[sorted_features, sorted_features]\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "plt.figure(figsize=(18, 14))\n",
        "plt.title('Correlation Matrix Sorted by Target Variable (SalePrice)', fontsize=16)\n",
        "\n",
        "# Draw heatmap with annotations and mask\n",
        "sns.heatmap(\n",
        "    sorted_corr,\n",
        "    cmap='coolwarm',\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    linewidths=0.5,\n",
        "    linecolor='gray',\n",
        "    cbar_kws={\"shrink\": 0.8},\n",
        "    square=True\n",
        ")\n",
        "\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.yticks(rotation=0, fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Target Column Analysis**"
      ],
      "metadata": {
        "id": "_DXd91_aXrwv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHZrGQCHC6qk"
      },
      "outputs": [],
      "source": [
        "# target column analysis\n",
        "sns.histplot(\n",
        "    train_df,\n",
        "    x=train_df['SalePrice'],color='royalblue'\n",
        ")\n",
        "# Show summary statistics\n",
        "print(\"Original Price Statistics:\")\n",
        "train_df['SalePrice'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "eWgx0Uu4EySe"
      },
      "outputs": [],
      "source": [
        "# transforming the target\n",
        "train_df['SalePrice'] = np.log1p(train_df['SalePrice'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-4V_qhiE2Sx"
      },
      "outputs": [],
      "source": [
        "# transformed target\n",
        "sns.histplot(\n",
        "    train_df,\n",
        "    x=train_df['SalePrice'],color='coral'\n",
        ")\n",
        "print(\"Log-Transformed Price Statistics:\")\n",
        "train_df['SalePrice'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Column Preparation | Data Preparation**"
      ],
      "metadata": {
        "id": "_WX7CBosX2ep"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "A8GEpDOBFH5f"
      },
      "outputs": [],
      "source": [
        "# Columns for ordinal encoding (have a natural order)\n",
        "ode_cols = [\n",
        "    'LotShape', 'LandContour', 'Utilities', 'LandSlope', 'BsmtQual', 'BsmtFinType1',\n",
        "    'CentralAir', 'Functional', 'FireplaceQu', 'GarageFinish', 'GarageQual',\n",
        "    'PavedDrive', 'ExterCond', 'KitchenQual', 'BsmtExposure', 'HeatingQC',\n",
        "    'ExterQual', 'BsmtCond'\n",
        "]\n",
        "\n",
        "# Columns for one-hot encoding (no natural order)\n",
        "ohe_cols = [\n",
        "    'Street', 'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
        "    'HouseStyle', 'RoofStyle', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
        "    'Foundation', 'Electrical', 'SaleType', 'MSZoning', 'SaleCondition',\n",
        "    'Heating', 'GarageType', 'RoofMatl'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "uBtzbZCYFWm-"
      },
      "outputs": [],
      "source": [
        "# Numeric columns (excluding the target)\n",
        "num_cols = train_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "if 'SalePrice' in num_cols:\n",
        "    num_cols.remove('SalePrice')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Pipeline**"
      ],
      "metadata": {
        "id": "ckBTiEUpZNsR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ieQTN7a4FOHo"
      },
      "outputs": [],
      "source": [
        "# Pipeline for numeric columns\n",
        "num_pipeline = Pipeline([\n",
        "    ('impute', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('var_thresh', VarianceThreshold(threshold=0.0))\n",
        "])\n",
        "\n",
        "# Pipeline for ordinal categorical columns\n",
        "ode_pipeline = Pipeline([\n",
        "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
        "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
        "])\n",
        "\n",
        "# Pipeline for nominal categorical columns\n",
        "ohe_pipeline = Pipeline([\n",
        "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
        "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "# ==== Column Transformer ====\n",
        "col_trans = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', num_pipeline, num_cols),\n",
        "        ('ord', ode_pipeline, ode_cols),\n",
        "        ('ohe', ohe_pipeline, ohe_cols)\n",
        "    ],\n",
        "    remainder='drop',\n",
        "    verbose_feature_names_out=False,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# ==== Final Preprocessing Pipeline ====\n",
        "preprocessor = Pipeline([\n",
        "    ('transform', col_trans),\n",
        "    ('feature_selector', SelectKBest(f_regression, k='all'))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation**"
      ],
      "metadata": {
        "id": "Ot_eNb-hHPgU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vgrWSDeyFYKp"
      },
      "outputs": [],
      "source": [
        "# preparing data\n",
        "x = train_df.drop('SalePrice', axis=1)\n",
        "y = train_df['SalePrice']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "K2EEqToyFe4R"
      },
      "outputs": [],
      "source": [
        "# Splitting the data\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building Models**"
      ],
      "metadata": {
        "id": "okDUUiO_HbJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define ALL Models with Parameter Grid\n",
        "models = {\n",
        "    # ========== Linear Models ==========\n",
        "    'Linear Regression': {\n",
        "        'model': LinearRegression(),\n",
        "        'params': {\n",
        "            'fit_intercept': [True, False],\n",
        "            'positive': [True, False]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'Ridge Regression': {\n",
        "        'model': Ridge(random_state=42),\n",
        "        'params': {\n",
        "            'alpha': [0.1, 0.5, 1.0, 2.0, 5.0],\n",
        "            'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'Lasso Regression': {\n",
        "        'model': Lasso(random_state=42),\n",
        "        'params': {\n",
        "            'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
        "            'selection': ['cyclic', 'random']\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'ElasticNet': {\n",
        "        'model': ElasticNet(random_state=42),\n",
        "        'params': {\n",
        "            'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
        "            'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
        "            'selection': ['cyclic', 'random']\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'Bayesian Ridge': {\n",
        "        'model': BayesianRidge(),\n",
        "        'params': {\n",
        "            'alpha_1': [1e-6, 1e-5, 1e-4],\n",
        "            'alpha_2': [1e-6, 1e-5, 1e-4],\n",
        "            'lambda_1': [1e-6, 1e-5, 1e-4],\n",
        "            'lambda_2': [1e-6, 1e-5, 1e-4]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'ARD Regression': {\n",
        "        'model': ARDRegression(),\n",
        "        'params': {\n",
        "            'alpha_1': [1e-6, 1e-5, 1e-4],\n",
        "            'alpha_2': [1e-6, 1e-5, 1e-4],\n",
        "            'lambda_1': [1e-6, 1e-5, 1e-4],\n",
        "            'lambda_2': [1e-6, 1e-5, 1e-4],\n",
        "            'threshold_lambda': [100, 1000, 10000]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'SGD Regressor': {\n",
        "        'model': SGDRegressor(random_state=42),\n",
        "        'params': {\n",
        "            'loss': ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
        "            'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "            'alpha': [0.0001, 0.001, 0.01],\n",
        "            'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive']\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'Huber Regressor': {\n",
        "        'model': HuberRegressor(),\n",
        "        'params': {\n",
        "            'epsilon': [1.1, 1.35, 1.5],\n",
        "            'alpha': [0.0001, 0.001, 0.01],\n",
        "            'max_iter': [100, 200, 300]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'Theil-Sen Regressor': {\n",
        "        'model': TheilSenRegressor(random_state=42),\n",
        "        'params': {\n",
        "            'max_subpopulation': [1000, 5000, 10000],\n",
        "            'n_subsamples': [None, 100, 200],\n",
        "            'max_iter': [100, 300, 500]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'RANSAC Regressor': {\n",
        "        'model': RANSACRegressor(random_state=42),\n",
        "        'params': {\n",
        "            'min_samples': [None, 0.1, 0.5, 0.9],\n",
        "            'residual_threshold': [None, 1.0, 2.0],\n",
        "            'max_trials': [50, 100, 200]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # ========== Tree-based Models ==========\n",
        "    'Decision Tree': {\n",
        "        'model': DecisionTreeRegressor(random_state=42),\n",
        "        'params': {\n",
        "            'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
        "            'max_depth': [None, 5, 10, 20, 30],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4],\n",
        "            'max_features': ['sqrt', 'log2', None]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'Random Forest': {\n",
        "        'model': RandomForestRegressor(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [None, 10, 20, 30],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4],\n",
        "            'max_features': ['sqrt', 'log2', None],\n",
        "            'bootstrap': [True, False]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'Extra Trees': {\n",
        "        'model': ExtraTreesRegressor(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200],\n",
        "            'criterion': ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'],\n",
        "            'max_features': ['sqrt', 'log2', None]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # ========== Boosting Models ==========\n",
        "    'Gradient Boosting': {\n",
        "        'model': GradientBoostingRegressor(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'subsample': [0.8, 1.0],\n",
        "            'min_samples_split': [2, 5]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'XGBoost': {\n",
        "        'model': XGBRegressor(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'max_depth': [3, 6, 9],\n",
        "            'subsample': [0.8, 1.0],\n",
        "            'colsample_bytree': [0.8, 1.0],\n",
        "            'gamma': [0, 0.1, 0.2],\n",
        "            'reg_alpha': [0, 0.1, 1],\n",
        "            'reg_lambda': [0, 0.1, 1]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'LightGBM': {\n",
        "        'model': LGBMRegressor(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'num_leaves': [31, 63, 127],\n",
        "            'max_depth': [-1, 10, 20],\n",
        "            'min_child_samples': [20, 50],\n",
        "            'reg_alpha': [0, 0.1, 1],\n",
        "            'reg_lambda': [0, 0.1, 1]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'CatBoost': {\n",
        "        'model': CatBoostRegressor(random_state=42, verbose=0),\n",
        "        'params': {\n",
        "            'iterations': [100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'depth': [4, 6, 8],\n",
        "            'l2_leaf_reg': [1, 3, 5]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'AdaBoost': {\n",
        "        'model': AdaBoostRegressor(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 1.0],\n",
        "            'loss': ['linear', 'square', 'exponential']\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'Hist Gradient Boosting': {\n",
        "        'model': HistGradientBoostingRegressor(random_state=42),\n",
        "        'params': {\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'max_iter': [100, 200],\n",
        "            'max_depth': [None, 10, 20],\n",
        "            'min_samples_leaf': [20, 50, 100]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # ========== SVM Models ==========\n",
        "    'SVR': {\n",
        "        'model': SVR(),\n",
        "        'params': {\n",
        "            'C': [0.1, 1, 10, 100],\n",
        "            'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
        "            'gamma': ['scale', 'auto', 0.1, 1],\n",
        "            'degree': [2, 3, 4],\n",
        "            'epsilon': [0.01, 0.1, 0.5]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'Linear SVR': {\n",
        "        'model': LinearSVR(random_state=42),\n",
        "        'params': {\n",
        "            'C': [0.1, 1, 10],\n",
        "            'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
        "            'dual': [True, False],\n",
        "            'epsilon': [0.01, 0.1, 0.5]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'NuSVR': {\n",
        "        'model': NuSVR(),\n",
        "        'params': {\n",
        "            'nu': [0.1, 0.5, 0.8],\n",
        "            'C': [0.1, 1, 10],\n",
        "            'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
        "            'gamma': ['scale', 'auto', 0.1, 1]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # ========== Nearest Neighbors ==========\n",
        "    'KNN Regressor': {\n",
        "        'model': KNeighborsRegressor(),\n",
        "        'params': {\n",
        "            'n_neighbors': [3, 5, 7, 9, 11],\n",
        "            'weights': ['uniform', 'distance'],\n",
        "            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "            'p': [1, 2]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'Radius Neighbors Regressor': {\n",
        "        'model': RadiusNeighborsRegressor(),\n",
        "        'params': {\n",
        "            'radius': [1.0, 2.0, 5.0],\n",
        "            'weights': ['uniform', 'distance'],\n",
        "            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # ========== Neural Networks ==========\n",
        "    'MLP Regressor': {\n",
        "        'model': MLPRegressor(random_state=42, early_stopping=True),\n",
        "        'params': {\n",
        "            'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
        "            'activation': ['relu', 'tanh', 'logistic'],\n",
        "            'alpha': [0.0001, 0.001, 0.01],\n",
        "            'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
        "            'learning_rate_init': [0.001, 0.01]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # ========== Gaussian Processes ==========\n",
        "    'Gaussian Process': {\n",
        "        'model': GaussianProcessRegressor(random_state=42),\n",
        "        'params': {\n",
        "            'kernel': [None,\n",
        "                      RBF(),\n",
        "                      ConstantKernel() * RBF(),\n",
        "                      RationalQuadratic()],\n",
        "            'alpha': [1e-10, 1e-5, 1e-2],\n",
        "            'normalize_y': [True, False]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # ========== Other Regressors ==========\n",
        "    'Kernel Ridge': {\n",
        "        'model': KernelRidge(),\n",
        "        'params': {\n",
        "            'alpha': [0.1, 1.0, 10.0],\n",
        "            'kernel': ['linear', 'rbf', 'polynomial'],\n",
        "            'gamma': [None, 0.1, 1.0],\n",
        "            'degree': [2, 3, 4]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'PLS Regression': {\n",
        "        'model': PLSRegression(),\n",
        "        'params': {\n",
        "            'n_components': [1, 2, 3, 5],\n",
        "            'scale': [True, False],\n",
        "            'max_iter': [500, 1000]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'Dummy Regressor': {\n",
        "        'model': DummyRegressor(),\n",
        "        'params': {\n",
        "            'strategy': ['mean', 'median', 'quantile', 'constant']\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "Qd2R49FwEwGL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Evaluation**"
      ],
      "metadata": {
        "id": "yENgKxJCHs1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Function\n",
        "def evaluate_regression_model(model, x_test, y_test):\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    metrics = {\n",
        "        'mse': mean_squared_error(y_test, y_pred),\n",
        "        'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "        'mae': mean_absolute_error(y_test, y_pred),\n",
        "        'r2': r2_score(y_test, y_pred),\n",
        "        'explained_variance': explained_variance_score(y_test, y_pred),\n",
        "        'max_error': max_error(y_test, y_pred),\n",
        "        'mape': mean_absolute_percentage_error(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    print(\"\\nRegression Metrics:\")\n",
        "    for name, value in metrics.items():\n",
        "        print(f\"{name}: {value:.4f}\")\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "HZM-Ck-HEwBs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training**"
      ],
      "metadata": {
        "id": "M_rJH0tTH4Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training\n",
        "results = {}\n",
        "best_models = {}\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for name, config in models.items():\n",
        "    print(f\"\\n{'=' * 50}\")\n",
        "    print(f\"Training and tuning {name}\")\n",
        "    print(f\"{'=' * 50}\")\n",
        "\n",
        "    try:\n",
        "        # Create pipeline with preprocessing and model\n",
        "        pipeline = Pipeline([\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('model', config['model'])\n",
        "        ])\n",
        "\n",
        "        # Update params to include pipeline prefix\n",
        "        params = {f'model__{key}': value for key, value in config['params'].items()}\n",
        "\n",
        "        grid_search = GridSearchCV(\n",
        "            pipeline,\n",
        "            param_grid=params,\n",
        "            cv=cv,\n",
        "            scoring='neg_mean_squared_error',\n",
        "            n_jobs=-1,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        grid_search.fit(x_train, y_train)\n",
        "        best_models[name] = grid_search.best_estimator_\n",
        "        results[name] = evaluate_regression_model(best_models[name], x_test, y_test)\n",
        "\n",
        "        print(f\"\\nBest parameters for {name}:\")\n",
        "        print(grid_search.best_params_)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error with {name}: {str(e)}\")\n",
        "        results[name] = {'error': str(e)}"
      ],
      "metadata": {
        "id": "InCuUe6TEv90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Comparison**"
      ],
      "metadata": {
        "id": "eS35Wo2cIQ3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Comparison\n",
        "comparison = pd.DataFrame.from_dict(results, orient='index')\n",
        "# Remove models that failed\n",
        "comparison = comparison[~comparison.index.isin([k for k, v in results.items() if 'error' in v])]\n",
        "print(comparison.sort_values(by='r2', ascending=False))"
      ],
      "metadata": {
        "id": "hndySQk1IP3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ensemble Methods**"
      ],
      "metadata": {
        "id": "LTDn5DDIIdCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select top models for ensemble\n",
        "top_models = comparison.nlargest(5, 'r2').index.tolist()\n",
        "\n",
        "# Create a list of base estimators for stacking\n",
        "estimators = [(name, best_models[name].named_steps['model']) for name in top_models]\n",
        "\n",
        "# Stacking Regressor\n",
        "stacking_reg = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', StackingRegressor(\n",
        "        estimators=estimators,\n",
        "        final_estimator=LinearRegression(),\n",
        "        cv=5,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "stacking_reg.fit(x_train, y_train)\n",
        "print(\"\\nStacking Regressor Performance:\")\n",
        "stacking_metrics = evaluate_regression_model(stacking_reg, x_test, y_test)\n",
        "results['Stacking'] = stacking_metrics\n",
        "\n",
        "# Voting Regressor\n",
        "voting_reg = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', VotingRegressor(\n",
        "        estimators=estimators,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "voting_reg.fit(x_train, y_train)\n",
        "print(\"\\nVoting Regressor Performance:\")\n",
        "voting_metrics = evaluate_regression_model(voting_reg, x_test, y_test)\n",
        "results['Voting'] = voting_metrics"
      ],
      "metadata": {
        "id": "UQzChB64Ev5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Comparison**"
      ],
      "metadata": {
        "id": "i1DysjzqJGtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Comparison\n",
        "\n",
        "print(\"\\nFinal Model Comparison:\")\n",
        "final_comparison = pd.DataFrame.from_dict(results, orient='index')\n",
        "final_comparison = final_comparison[~final_comparison.index.isin([k for k, v in results.items() if 'error' in v])]\n",
        "print(final_comparison.sort_values(by='r2', ascending=False))"
      ],
      "metadata": {
        "id": "bBN36G_9Ev0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving The Model**"
      ],
      "metadata": {
        "id": "LJAkWmYvJf3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Best Model\n",
        "best_model_name = final_comparison['r2'].idxmax()\n",
        "best_model = best_models.get(best_model_name,\n",
        "                             stacking_reg if best_model_name == 'Stacking' else voting_reg)\n",
        "\n",
        "print(f\"\\nBest model is: {best_model_name}\")\n",
        "\n",
        "# Save the best model\n",
        "from joblib import dump\n",
        "\n",
        "dump(best_model, 'best_regressor_model.joblib')\n",
        "\n",
        "# Save all results to CSV\n",
        "final_comparison.sort_values(by='r2', ascending=False).to_csv('regression_model_comparison.csv')\n",
        "\n",
        "print(\"\\nModel training and evaluation complete!\")"
      ],
      "metadata": {
        "id": "icW9R2siEvej"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmb5uAHR3fnZC43VAzUo7P",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}